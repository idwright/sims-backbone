#!/bin/bash

export INPUT1_STAGING_DIR=/app/upload/input
export OUTPUT1_STAGING_DIR=/app/upload/output

IMPORT_CONFIG=import_config.json
aws s3 cp ${IMPORT_SOURCE}/config.json ${IMPORT_CONFIG}

CMIS_CONFIG=cmis_config.json
aws s3 cp ${IMPORT_SOURCE}/cmis_config.json ${CMIS_CONFIG}

OUTFILE=${OUTPUT1_STAGING_DIR}/import_$(date +%Y-%m-%d-%H%M%S).log
export PYTHONPATH=/app/upload:/app/python_client:/app/server:/app/server/bb_server

mkdir -p $(dirname ${OUTFILE})
# Redirect stdout ( > ) into a named pipe ( >() ) running "tee"
exec &> >(tee -i ${OUTFILE})

ARCHIVE_DIR=${OUTPUT1_STAGING_DIR}/archive
test -d ${ARCHIVE_DIR} || mkdir -p ${ARCHIVE_DIR}

aws s3 sync ${IMPORT_SOURCE} ${INPUT1_STAGING_DIR}/import
shopt -s nullglob
for i in ${INPUT1_STAGING_DIR}/import/roma/*
do
    if [ $i != "_dummy" ]
    then
        test -d ${ARCHIVE_DIR}/roma || mkdir -p ${ARCHIVE_DIR}/roma
        INSTANCE=$(basename ${i} | awk -F_ '{print $1}')
        ROMA_LOG=${OUTPUT1_STAGING_DIR}/${INSTANCE}_$(date +%Y-%m-%d-%H%M%S).log
        python3 upload_roma.py ${IMPORT_CONFIG} $i 2>&1 | tee -i ${ROMA_LOG}
        if [ -s ${ROMA_LOG} ]
        then
            python3 upload_log.py ${CMIS_CONFIG} ${ENVIRON} ${ROMA_LOG}
        fi
        mv $i ${ARCHIVE_DIR}/roma/$(basename ${i})
    fi
done
for i in ${INPUT1_STAGING_DIR}/import/access/*
do
    if [ ${i} != "_dummy" ]
    then
        test -d ${ARCHIVE_DIR}/access || mkdir -p ${ARCHIVE_DIR}/access
        INSTANCE=$(basename ${i})
        ACCESS_LOG=${OUTPUT1_STAGING_DIR}/${INSTANCE}_$(date +%Y-%m-%d-%H%M%S).log
        JAR="lims-sims/target/lims-transform-1.0-SNAPSHOT.jar"
        if [ ! -f ${JAR} ]
        then
            (cd $(dirname ${JAR})/..;
            mvn package)
        fi
        java -classpath ${JAR}/../../util -jar ${JAR} net.malariagen.sims.lims.Transform ${i} ${i}.csv
        python3 uploader.py ${i}.csv access.json ${IMPORT_CONFIG} 2>&1 | tee -i ${ACCESS_LOG}
        if [ -s ${ACCESS_LOG} ]
        then
            python3 upload_log.py ${CMIS_CONFIG} ${ENVIRON} ${ACCESS_LOG}
        fi
        test -d ${ARCHIVE_DIR}/access && mkdir ${ARCHIVE_DIR}/access
        mv $i ${ARCHIVE_DIR}/access/$(basename ${i})
    fi
METADATA_LOG=${OUTPUT1_STAGING_DIR}/metadata_$(date +%Y-%m-%d-%H%M%S).log
python3 set_taxa.py ${IMPORT_CONFIG} | tee -i ${METADATA_LOG}
python3 set_studies.py ${IMPORT_CONFIG} ${CMIS_CONFIG} | tee -i ${METADATA_LOG}
aws s3 sync --delete ${INPUT1_STAGING_DIR}/import ${IMPORT_SOURCE} 
aws s3 sync ${OUTPUT1_STAGING_DIR} ${IMPORT_DEST}
